{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Augmentation\n",
    "In this notebook, 3 Variants of SMOTE are compared for data augmentation. The training dataset is being scaled to 2000 observations.\n",
    "Manual noise is induced for synthetic samples in order to improve robustness and prevent overfitting."
   ],
   "id": "a495a7d3355a7b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, KMeansSMOTE\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "55b421877493f0e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Functions and Utils",
   "id": "7e5a22bcff10b252"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a custom colormap\n",
    "colors = [\n",
    "    (0.945, 0.980, 0.733),\n",
    "    (0.263, 0.671, 0.702),\n",
    "    (0.137, 0.294, 0.620),\n",
    "]\n",
    "\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"custom_diverging\", colors, N=256)\n",
    "\n",
    "# Custom formatter for correlation values\n",
    "def custom_fmt(val):\n",
    "    abs_val = abs(val)\n",
    "    if abs_val == 1.0:\n",
    "        return \"1.0\"\n",
    "    else:\n",
    "        return f\".{int(abs_val * 100):02d}\"\n",
    "\n",
    "# Function to create and save correlation heatmap\n",
    "def plot_correlation_heatmap(df, title, filename):\n",
    "    if 'performance_class' in df.columns:\n",
    "        data = df.drop(columns=[\"performance_class\"])\n",
    "    else:\n",
    "        data = df\n",
    "    \n",
    "    corr_matrix = data.corr()\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    heatmap = sns.heatmap(\n",
    "        corr_matrix,\n",
    "        mask=np.triu(np.ones_like(corr_matrix, dtype=bool), k=1),\n",
    "        annot=True,\n",
    "        fmt=\"\",\n",
    "        annot_kws={\"size\": 8},\n",
    "        cmap=custom_cmap,\n",
    "        vmin=-1, vmax=1,\n",
    "        center=0,\n",
    "        linewidths=0.2,\n",
    "        cbar_kws={\"shrink\": 0.8}\n",
    "    )\n",
    "    \n",
    "    for text in heatmap.texts:\n",
    "        text_value = float(text.get_text().replace('−', '-'))\n",
    "        text.set_text(custom_fmt(text_value))\n",
    "    \n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.subplots_adjust(bottom=0.3)\n",
    "    \n",
    "    plt.savefig(filename, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "# Function to calculate correlation difference metrics\n",
    "def calculate_correlation_difference(original_corr, augmented_corr):\n",
    "    # Calculate absolute differences\n",
    "    diff_matrix = np.abs(original_corr - augmented_corr)\n",
    "    \n",
    "    # Calculate mean absolute difference (excluding diagonal)\n",
    "    mask = ~np.eye(original_corr.shape[0], dtype=bool)\n",
    "    mean_abs_diff = diff_matrix.values[mask].mean()\n",
    "    \n",
    "    # Calculate max absolute difference\n",
    "    max_abs_diff = diff_matrix.values[mask].max()\n",
    "    \n",
    "    return {\n",
    "        'mean_abs_diff': mean_abs_diff,\n",
    "        'max_abs_diff': max_abs_diff,\n",
    "        'diff_matrix': diff_matrix\n",
    "    }\n",
    "\n",
    "# Function to plot feature distributions\n",
    "def plot_feature_distributions(original_df, augmented_df, method_name):\n",
    "    if 'performance_class' in original_df.columns:\n",
    "        original_features = original_df.drop(columns=['performance_class'])\n",
    "        augmented_features = augmented_df.drop(columns=['performance_class'])\n",
    "    else:\n",
    "        original_features = original_df\n",
    "        augmented_features = augmented_df\n",
    "    \n",
    "    # Select a subset of features if there are too many\n",
    "    features_to_plot = original_features.columns[:min(10, len(original_features.columns))]\n",
    "    \n",
    "    fig, axes = plt.subplots(len(features_to_plot), 2, figsize=(15, 4*len(features_to_plot)))\n",
    "    \n",
    "    for i, feature in enumerate(features_to_plot):\n",
    "        # Original data distribution\n",
    "        sns.histplot(original_features[feature], kde=True, color='royalblue', ax=axes[i, 0])\n",
    "        axes[i, 0].set_title(f\"{feature} (Original)\")\n",
    "        \n",
    "        # Augmented data distribution\n",
    "        sns.histplot(augmented_features[feature], kde=True, color='orange', ax=axes[i, 1])\n",
    "        axes[i, 1].set_title(f\"{feature} ({method_name})\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../data/figures/feature_distributions_{method_name}.pdf\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "def apply_smote_with_noise(X, y, augmenter, noise_level=0.05, random_state=42):\n",
    "    \"\"\"\n",
    "    Apply SMOTE and add random noise to the synthetic samples\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : DataFrame\n",
    "        Features\n",
    "    y : Series\n",
    "        Target labels\n",
    "    augmenter : object\n",
    "        SMOTE, BorderlineSMOTE, or KMeansSMOTE object\n",
    "    noise_level : float\n",
    "        Standard deviation of Gaussian noise as a fraction of feature range\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_resampled : DataFrame\n",
    "        Features with original and noisy synthetic samples\n",
    "    y_resampled : Series\n",
    "        Corresponding labels\n",
    "    \"\"\"\n",
    "    # Record original samples to identify synthetic ones later\n",
    "    original_indices = X.index.tolist()\n",
    "    \n",
    "    # Apply SMOTE\n",
    "    X_smote, y_smote = augmenter.fit_resample(X, y)\n",
    "    \n",
    "    # Convert to DataFrame and Series to maintain column names and other metadata\n",
    "    if not isinstance(X_smote, pd.DataFrame):\n",
    "        X_smote = pd.DataFrame(X_smote, columns=X.columns)\n",
    "    if not isinstance(y_smote, pd.Series):\n",
    "        y_smote = pd.Series(y_smote, name=y.name)\n",
    "    \n",
    "    synthetic_mask = ~X_smote.index.isin(original_indices)\n",
    "    synthetic_indices = X_smote.index[synthetic_mask]\n",
    "    \n",
    "    # Generate noise proportional to each feature's range\n",
    "    feature_ranges = X.max() - X.min()\n",
    "    np.random.seed(random_state)\n",
    "    noise = np.random.normal(\n",
    "        0, \n",
    "        noise_level, \n",
    "        size=(len(synthetic_indices), X.shape[1])\n",
    "    ) * feature_ranges.values\n",
    "    \n",
    "    # Add noise to synthetic samples\n",
    "    X_smote.loc[synthetic_indices] += noise\n",
    "    \n",
    "    return X_smote, y_smote"
   ],
   "id": "ce353bcffb6520b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run Data Augmentation",
   "id": "4438b6b09afa1d03"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Main function to run the augmentation\n",
    "def run_augmentation_experiment(input_file):\n",
    "    # ---- LOAD DATASET ----\n",
    "\n",
    "    df = pd.read_csv(input_file)\n",
    "    X = df.drop(columns=[\"performance_class\"])\n",
    "    y = df[\"performance_class\"]\n",
    "    \n",
    "    # ---- SPLIT DATASET ----\n",
    "    \n",
    "    print(\"Splitting dataset into train/test sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    \n",
    "    # Save split datasets for consistent splits\n",
    "    print(\"Saving train/test splits...\")\n",
    "    joblib.dump(X_train, \"../data/splits/X_train.pkl\")\n",
    "    joblib.dump(y_train, \"../data/splits/y_train.pkl\")\n",
    "    joblib.dump(X_test, \"../data/splits/X_test.pkl\")\n",
    "    joblib.dump(y_test, \"../data/splits/y_test.pkl\")\n",
    "    \n",
    "    print(\"Train/test split saved successfully!\")\n",
    "    \n",
    "    # Create a DataFrame from the training data for correlation analysis\n",
    "    df_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "    df_train[\"performance_class\"] = y_train\n",
    "    \n",
    "    # Save original training data correlation matrix\n",
    "    original_corr = plot_correlation_heatmap(\n",
    "        X_train, \n",
    "        \"Original Training Data Correlation Matrix\", \n",
    "        \"../data/figures/original_train_correlation_heatmap.pdf\"\n",
    "    )\n",
    "    \n",
    "    # Augmentation methods\n",
    "    augmentation_methods = {\n",
    "        'SMOTE_k5': SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42),\n",
    "        'BorderlineSMOTE': BorderlineSMOTE(sampling_strategy='auto', random_state=42),\n",
    "        'KMeansSMOTE': KMeansSMOTE(sampling_strategy='auto', cluster_balance_threshold=0.1, random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    augmented_dfs = {}\n",
    "    \n",
    "    # Run each augmentation method on the training data\n",
    "    for method_name, augmenter in augmentation_methods.items():\n",
    "        print(f\"\\nRunning {method_name} on training data only...\")\n",
    "        \n",
    "        try:\n",
    "            # ---- Step 1: Initial augmentation to balance classes ----\n",
    "            \n",
    "            X_train_resampled_1, y_train_resampled_1 = apply_smote_with_noise(\n",
    "                X_train, y_train, augmenter, noise_level=0.05, random_state=42\n",
    "            )\n",
    "            df_train_balanced = pd.DataFrame(X_train_resampled_1, columns=X_train.columns)\n",
    "            df_train_balanced[\"performance_class\"] = y_train_resampled_1\n",
    "                        \n",
    "            print(f\"Training dataset size after initial {method_name}:\", df_train_balanced.shape)\n",
    "            print(y_train_resampled_1.value_counts())\n",
    "                        \n",
    "            # ---- Step 2: Large scale augmentation to achieve 2000 samples in total ----\n",
    "            \n",
    "            target_count_per_class = 1000\n",
    "            sampling_strategy_large = {}\n",
    "            for cls in y_train_resampled_1.unique():\n",
    "                sampling_strategy_large[cls] = target_count_per_class\n",
    "                        \n",
    "            augmenter_params = {}\n",
    "            if method_name == 'KMeansSMOTE':\n",
    "                augmenter_params['cluster_balance_threshold'] = 0.1\n",
    "                        \n",
    "            if method_name == 'SMOTE_k5':\n",
    "                augmenter_large = SMOTE(sampling_strategy=sampling_strategy_large, k_neighbors=5, random_state=44, **augmenter_params)\n",
    "            elif method_name == 'BorderlineSMOTE':\n",
    "                augmenter_large = BorderlineSMOTE(sampling_strategy=sampling_strategy_large, random_state=44, **augmenter_params)\n",
    "            elif method_name == 'KMeansSMOTE':\n",
    "                augmenter_large = KMeansSMOTE(sampling_strategy=sampling_strategy_large, random_state=44, **augmenter_params)\n",
    "                        \n",
    "            X_train_balanced = df_train_balanced.drop(columns=[\"performance_class\"])\n",
    "            y_train_balanced = df_train_balanced[\"performance_class\"]\n",
    "            X_train_resampled_large, y_train_resampled_large = apply_smote_with_noise(\n",
    "                X_train_balanced, y_train_balanced, augmenter_large, noise_level=0.05, random_state=44\n",
    "            )\n",
    "                        \n",
    "            df_train_large = pd.DataFrame(X_train_resampled_large, columns=X_train.columns)\n",
    "            df_train_large[\"performance_class\"] = y_train_resampled_large\n",
    "                        \n",
    "            print(f\"Large training dataset size after {method_name}:\", df_train_large.shape)\n",
    "            print(y_train_resampled_large.value_counts())\n",
    "            \n",
    "            # ---- SAVE AUGMENTED TRAINING DATA ----\n",
    "            \n",
    "            joblib.dump(X_train_resampled_large, f\"../data/splits/X_train_augmented_{method_name}.pkl\")\n",
    "            joblib.dump(y_train_resampled_large, f\"../data/splits/y_train_augmented_{method_name}.pkl\")\n",
    "            print(f\"Augmented training data saved as pkl files for {method_name}\")\n",
    "                        \n",
    "            # ---- SAVE AUGMENTED CORRELATION MATRIX ----\n",
    "            \n",
    "            augmented_corr = plot_correlation_heatmap(\n",
    "                df_train_large.drop(columns=[\"performance_class\"]),\n",
    "                f\"{method_name} Augmented Training Data Correlation Matrix\",\n",
    "                f\"../data/figures/{method_name}_train_correlation_heatmap.pdf\"\n",
    "            )\n",
    "                        \n",
    "            # ---- CALCULATE CORRELATION DIFFERENCES COMPARED TO ORIGINAL FEATURE CORRELATIONS ----\n",
    "            \n",
    "            diff_results = calculate_correlation_difference(original_corr, augmented_corr)\n",
    "                        \n",
    "            # ---- SAVE RESULTS ----\n",
    "            \n",
    "            results[method_name] = diff_results\n",
    "            augmented_dfs[method_name] = df_train_large\n",
    "                        \n",
    "            # ---- PLOT CORRELATION DIFFERENCE MATRICES ----\n",
    "            \n",
    "            plt.figure(figsize=(12, 10))\n",
    "            heatmap = sns.heatmap(\n",
    "                diff_results['diff_matrix'],\n",
    "                mask=np.triu(np.ones_like(diff_results['diff_matrix'], dtype=bool), k=1),\n",
    "                annot=True,\n",
    "                fmt=\"\",\n",
    "                annot_kws={\"size\": 8},\n",
    "                cmap=custom_cmap,\n",
    "                vmin=0,\n",
    "                vmax=0.5\n",
    "            )\n",
    "            for text in heatmap.texts:\n",
    "                text_value = float(text.get_text().replace('−', '-'))\n",
    "                text.set_text(custom_fmt(text_value))\n",
    "            plt.title(f'Correlation Difference: {method_name}\\nMean: {diff_results[\"mean_abs_diff\"]:.4f}, Max: {diff_results[\"max_abs_diff\"]:.4f}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"../data/figures/{method_name}_diff_heatmap.pdf\", dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {method_name}: {str(e)}\")\n",
    "    \n",
    "    # ---- SAVE CORRELATION DIFFERENCE METRICS ----\n",
    "    \n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Method': list(results.keys()),\n",
    "        'Mean_Absolute_Difference': [results[method]['mean_abs_diff'] for method in results],\n",
    "        'Max_Absolute_Difference': [results[method]['max_abs_diff'] for method in results]\n",
    "    })\n",
    "    \n",
    "    metrics_df.to_csv(\"../data/correlation_difference_metrics.csv\", index=False)\n",
    "    print(\"\\nCorrelation difference metrics saved to 'correlation_difference_metrics.csv'\")\n",
    "    \n",
    "    # Find the best method\n",
    "    if results:\n",
    "        best_method = min(results.items(), key=lambda x: x[1]['mean_abs_diff'])[0]\n",
    "        print(f\"\\nBest method based on correlation preservation: {best_method}\")\n",
    "        \n",
    "        # ---- PLOT FEATURE DISTRIBUTION FOR THE BEST SMOTE AUGMENTATION ALGORITHM ----\n",
    "        if best_method in augmented_dfs:\n",
    "            df_train_for_plot = pd.DataFrame(X_train, columns=X_train.columns)\n",
    "            df_train_for_plot[\"performance_class\"] = y_train\n",
    "            \n",
    "            plot_feature_distributions(df_train_for_plot, augmented_dfs[best_method], best_method)\n",
    "            print(f\"Feature distribution plot saved for {best_method}\")\n",
    "            \n",
    "            best_df = augmented_dfs[best_method]\n",
    "            X_train_augmented_best = best_df.drop(columns=[\"performance_class\"])\n",
    "            y_train_augmented_best = best_df[\"performance_class\"]\n",
    "            \n",
    "            # ---- SAVE BEST AUGMENTED TRAINING DATA ----\n",
    "            \n",
    "            joblib.dump(X_train_augmented_best, f\"../data/splits/X_train_augmented_best.pkl\")\n",
    "            joblib.dump(y_train_augmented_best, f\"../data/splits/y_train_augmented_best.pkl\")\n",
    "            print(f\"Best augmented training data ({best_method}) saved as pickle files\")\n",
    "    \n",
    "    return results, augmented_dfs\n",
    "\n",
    "# ---- RUN THE CODE ----\n",
    "if __name__ == \"__main__\":\n",
    "    results, augmented_dfs = run_augmentation_experiment(\"../data/features/filtered_labeled_feature_matrix.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
