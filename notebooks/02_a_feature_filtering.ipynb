{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648b928bc55dda9f",
   "metadata": {},
   "source": [
    "# Feature Filtering\n",
    "\n",
    "This notebook performs an initial selection of features for model training. Since high correlations between features can lead to overfitting, the dataset is first analyzed for correlations, and one feature from each highly correlated pair is removed.\n",
    "\n",
    "A more specific feature space reduction will be part of the model training process, as part of hyperparameter tuning. The reason being, that the optimal feature space dimension can be highly dependent on the model used."
   ]
  },
  {
   "cell_type": "code",
   "id": "451a5f80492fe625",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4d76a0b176b397c",
   "metadata": {},
   "source": [
    "# ---- LOAD DATA ----\n",
    "\n",
    "df = pd.read_csv(\"../data/processed/labeled_feature_matrix.csv\")\n",
    "X = df.drop(columns=[\"performance_class\"])\n",
    "y = df[\"performance_class\"]\n",
    "\n",
    "value_counts = y.value_counts()\n",
    "print(value_counts)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "763d51ab64518031",
   "metadata": {},
   "source": [
    "# Filter by Variance\n",
    "\n",
    "This aimes to remove features with true zero variance, as they don't contain useful information."
   ]
  },
  {
   "cell_type": "code",
   "id": "25ecae89306d78",
   "metadata": {},
   "source": [
    "variance_threshold = 0\n",
    "var_selector = VarianceThreshold(threshold=variance_threshold)\n",
    "X_var_filtered = var_selector.fit_transform(X)\n",
    "var_scores = var_selector.variances_\n",
    "\n",
    "feature_names = X.columns\n",
    "variance_df = pd.DataFrame({\"Feature\": feature_names, \"Variance\": var_scores})\n",
    "variance_df = variance_df.sort_values(by=\"Variance\", ascending=True)\n",
    "\n",
    "# Features with low variance\n",
    "weak_features = variance_df[variance_df[\"Variance\"] == variance_threshold][\"Feature\"].tolist()\n",
    "print(\"Low variance features (Variance < 0.01):\", weak_features)\n",
    "\n",
    "# Show retained features\n",
    "selected_var_features = X.columns[var_selector.get_support()]\n",
    "print(f\"Retained features after variance based filtering : {list(selected_var_features)}\")\n",
    "\n",
    "# Update X with the filtered features\n",
    "X = pd.DataFrame(X_var_filtered, columns=selected_var_features)\n",
    "\n",
    "# ---- PLOT ----\n",
    "\n",
    "plt.figure(figsize=(6, 22))\n",
    "bars = plt.barh(variance_df[\"Feature\"], variance_df[\"Variance\"], color='#296099')\n",
    "plt.xlabel(\"Variance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "for bar in bars:\n",
    "    if bar.get_width() < 55000:\n",
    "        plt.text(bar.get_width() + 200, bar.get_y() + bar.get_height()/2, f'{bar.get_width():.10f}', va='center')\n",
    "\n",
    "plt.savefig(\"../data/figures/02_feature_correlation_chart.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## FEATURE SCALING\n",
    "After filtering out features with low variance, the remaining features are scaled to have a mean of 0 and a standard deviation of 1. This is done to ensure that all features have the same scale, which is important for many machine learning algorithms."
   ],
   "id": "c2dfc77e7d67f85b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---- FEATURE SCALING ----\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)"
   ],
   "id": "10620b7c959d0365",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c8fde8489d738273",
   "metadata": {},
   "source": [
    "# Filter Highly Correlated Features\n",
    "\n",
    "Features, that are highly correlated to each other are redundant and can lead to overfitting. To avoid this, we remove one feature from each highly correlated pair."
   ]
  },
  {
   "cell_type": "code",
   "id": "fc7a1e4442fccb88",
   "metadata": {},
   "source": [
    "# Calculate correlation matrix\n",
    "corr_matrix = X_scaled.corr()\n",
    "\n",
    "# Create a custom colormap\n",
    "def custom_colormap():\n",
    "    cmap = sns.diverging_palette(610, 610, as_cmap=True, center=\"dark\", s=90)\n",
    "    cmap_colors = cmap(np.arange(cmap.N))\n",
    "    custom_color = np.array([106/255, 160/255, 209/255, 1]) \n",
    "    white = np.array([1, 1, 1, 1])\n",
    "    threshold_high = int(0.95 * (cmap.N - 1))\n",
    "    threshold_low = int(-0.96 * (cmap.N - 1))\n",
    "    cmap_colors[threshold_high:] = custom_color  # Set values > 0.9 to custom color\n",
    "    cmap_colors[:threshold_low] = custom_color   # Set values < -0.9 to custom color\n",
    "    return ListedColormap(cmap_colors)\n",
    "\n",
    "# Find feature pairs with correlation < -0.9 or > 0.9, excluding the diagonal\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if corr_matrix.iloc[i, j] < -0.9 or corr_matrix.iloc[i, j] > 0.9:\n",
    "            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "heatmap = sns.heatmap(corr_matrix, annot=False, cmap=custom_colormap(), fmt=\".2f\")\n",
    "plt.subplots_adjust(bottom=0.3)\n",
    "print(high_corr_pairs)\n",
    "\n",
    "# Remove one feature from each pair if the second feature remains in X_scaled\n",
    "features_to_remove = set()\n",
    "for pair in high_corr_pairs:\n",
    "    if pair[0] in X_scaled.columns and pair[1] in X_scaled.columns:\n",
    "        features_to_remove.add(pair[0])\n",
    "        \n",
    "# ---- PLOT ----\n",
    "\n",
    "plt.savefig(\"../data/figures/02_feature_correlation_heatmap.pdf\", \n",
    "            bbox_inches='tight', \n",
    "            dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Actually perform the dropping after saving the figure\n",
    "for feature in features_to_remove:\n",
    "    if feature in X_scaled.columns:\n",
    "        X_scaled = X_scaled.drop(columns=feature)\n",
    "\n",
    "print(f\"Removed features: {features_to_remove}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Calculate ANOVA Scores\n",
    "Anova scores are calculated to determine the importance of each feature for the classification task. The scores are later compared to the SHAP Feature Importance scores to evaluate the feature selection process, and assess if ANOVA F-Scores are a good indicator for feature importance."
   ],
   "id": "884d1246a08fc51d"
  },
  {
   "cell_type": "code",
   "id": "cbadca33b3f55681",
   "metadata": {},
   "source": [
    "anova_selector = SelectKBest(score_func=f_classif, k=\"all\")\n",
    "X_anova = anova_selector.fit_transform(X_scaled, y)\n",
    "selected_features_anova = X_scaled.columns[anova_selector.get_support()]\n",
    "\n",
    "anova_scores = anova_selector.scores_\n",
    "feature_names = X_scaled.columns\n",
    "\n",
    "anova_df = pd.DataFrame({\"Feature\": feature_names, \"ANOVA Score\": anova_scores})\n",
    "anova_df = anova_df.sort_values(by=\"ANOVA Score\", ascending=False)\n",
    "\n",
    "# Weak ANOVA features\n",
    "weak_features = anova_df[anova_df[\"ANOVA Score\"] < 0.5][\"Feature\"].tolist()\n",
    "print(\"Schwache Features (ANOVA Score < 0.5):\", weak_features)\n",
    "\n",
    "# ---- PLOT ----\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(feature_names, anova_scores, color='#296099')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"ANOVA Score\")\n",
    "plt.savefig(\"../data/figures/02_feature_anova_chart.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SAVE FILTERED FEATURE MATRIX",
   "id": "d79dda1f9c4dd2d1"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# ---- SAVE FILTERED FEATURE MATRIX ----\n",
    "\n",
    "X_scaled_with_y = pd.concat([X_scaled, y.reset_index(drop=True)], axis=1)\n",
    "X_scaled_with_y.to_csv(\"../data/features/filtered_labeled_feature_matrix.csv\", index=False)\n",
    "print(\"Feature Selection finished! Filtered FeatureSpace was saved to data/features/selected_features.csv\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ba-thesis-py3.10",
   "language": "python",
   "name": "ba-thesis-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
