{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T21:48:14.087218Z",
     "start_time": "2025-03-17T21:48:14.059648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, KMeansSMOTE\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "55b421877493f0e9",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T21:48:14.116025Z",
     "start_time": "2025-03-17T21:48:14.102145Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a custom colormap\n",
    "colors = [\n",
    "    (0.945, 0.980, 0.733),  # light yellow-green\n",
    "    (0.263, 0.671, 0.702),  # teal\n",
    "    (0.137, 0.294, 0.620),  # navy blue\n",
    "]\n",
    "\n",
    "custom_cmap = LinearSegmentedColormap.from_list(\"custom_diverging\", colors, N=256)\n",
    "\n",
    "# Custom formatter for correlation values\n",
    "def custom_fmt(val):\n",
    "    abs_val = abs(val)\n",
    "    if abs_val == 1.0:\n",
    "        return \"1.0\"\n",
    "    else:\n",
    "        return f\".{int(abs_val * 100):02d}\"\n",
    "\n",
    "# Function to create and save correlation heatmap\n",
    "def plot_correlation_heatmap(df, title, filename):\n",
    "    # Drop the target variable for correlation calculation\n",
    "    if 'performance_class' in df.columns:\n",
    "        data = df.drop(columns=[\"performance_class\"])\n",
    "    else:\n",
    "        data = df\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = data.corr()\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    \n",
    "    # Create heatmap\n",
    "    heatmap = sns.heatmap(\n",
    "        corr_matrix,\n",
    "        mask=np.triu(np.ones_like(corr_matrix, dtype=bool), k=1),\n",
    "        annot=True,\n",
    "        fmt=\"\",\n",
    "        annot_kws={\"size\": 8},\n",
    "        cmap=custom_cmap,\n",
    "        vmin=-1, vmax=1,\n",
    "        center=0,\n",
    "        linewidths=0.2,\n",
    "        cbar_kws={\"shrink\": 0.8}\n",
    "    )\n",
    "    \n",
    "    # Apply custom formatter\n",
    "    for text in heatmap.texts:\n",
    "        text_value = float(text.get_text().replace('−', '-'))\n",
    "        text.set_text(custom_fmt(text_value))\n",
    "    \n",
    "    # Add title\n",
    "    plt.title(title, fontsize=14)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.subplots_adjust(bottom=0.3)\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(filename, bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return corr_matrix\n",
    "\n",
    "# Function to calculate correlation difference metrics\n",
    "def calculate_correlation_difference(original_corr, augmented_corr):\n",
    "    # Calculate absolute differences\n",
    "    diff_matrix = np.abs(original_corr - augmented_corr)\n",
    "    \n",
    "    # Calculate mean absolute difference (excluding diagonal)\n",
    "    mask = ~np.eye(original_corr.shape[0], dtype=bool)\n",
    "    mean_abs_diff = diff_matrix.values[mask].mean()\n",
    "    \n",
    "    # Calculate max absolute difference\n",
    "    max_abs_diff = diff_matrix.values[mask].max()\n",
    "    \n",
    "    return {\n",
    "        'mean_abs_diff': mean_abs_diff,\n",
    "        'max_abs_diff': max_abs_diff,\n",
    "        'diff_matrix': diff_matrix\n",
    "    }\n",
    "\n",
    "# Function to plot feature distributions\n",
    "def plot_feature_distributions(original_df, augmented_df, method_name):\n",
    "    if 'performance_class' in original_df.columns:\n",
    "        original_features = original_df.drop(columns=['performance_class'])\n",
    "        augmented_features = augmented_df.drop(columns=['performance_class'])\n",
    "    else:\n",
    "        original_features = original_df\n",
    "        augmented_features = augmented_df\n",
    "    \n",
    "    # Select a subset of features if there are too many\n",
    "    features_to_plot = original_features.columns[:min(10, len(original_features.columns))]\n",
    "    \n",
    "    fig, axes = plt.subplots(len(features_to_plot), 2, figsize=(15, 4*len(features_to_plot)))\n",
    "    \n",
    "    for i, feature in enumerate(features_to_plot):\n",
    "        # Original data distribution\n",
    "        sns.histplot(original_features[feature], kde=True, color='royalblue', ax=axes[i, 0])\n",
    "        axes[i, 0].set_title(f\"{feature} (Original)\")\n",
    "        \n",
    "        # Augmented data distribution\n",
    "        sns.histplot(augmented_features[feature], kde=True, color='orange', ax=axes[i, 1])\n",
    "        axes[i, 1].set_title(f\"{feature} ({method_name})\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../data/figures/feature_distributions_{method_name}.pdf\", dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "# First, add this function to your code, right before the run_augmentation_experiment function\n",
    "\n",
    "def apply_smote_with_noise(X, y, augmenter, noise_level=0.05, random_state=42):\n",
    "    \"\"\"\n",
    "    Apply SMOTE and add random noise to the synthetic samples\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : DataFrame\n",
    "        Features\n",
    "    y : Series\n",
    "        Target labels\n",
    "    augmenter : object\n",
    "        SMOTE, BorderlineSMOTE, or KMeansSMOTE object\n",
    "    noise_level : float\n",
    "        Standard deviation of Gaussian noise as a fraction of feature range\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_resampled : DataFrame\n",
    "        Features with original and noisy synthetic samples\n",
    "    y_resampled : Series\n",
    "        Corresponding labels\n",
    "    \"\"\"\n",
    "    # Record original samples to identify synthetic ones later\n",
    "    original_indices = X.index.tolist()\n",
    "    \n",
    "    # Apply SMOTE\n",
    "    X_smote, y_smote = augmenter.fit_resample(X, y)\n",
    "    \n",
    "    # Convert to DataFrame and Series to maintain column names and other metadata\n",
    "    if not isinstance(X_smote, pd.DataFrame):\n",
    "        X_smote = pd.DataFrame(X_smote, columns=X.columns)\n",
    "    if not isinstance(y_smote, pd.Series):\n",
    "        y_smote = pd.Series(y_smote, name=y.name)\n",
    "    \n",
    "    # Identify which samples are synthetic (not in original data)\n",
    "    synthetic_mask = ~X_smote.index.isin(original_indices)\n",
    "    synthetic_indices = X_smote.index[synthetic_mask]\n",
    "    \n",
    "    # Calculate feature ranges for scaling noise\n",
    "    feature_ranges = X.max() - X.min()\n",
    "    \n",
    "    # Generate noise proportional to each feature's range\n",
    "    np.random.seed(random_state)\n",
    "    noise = np.random.normal(\n",
    "        0, \n",
    "        noise_level, \n",
    "        size=(len(synthetic_indices), X.shape[1])\n",
    "    ) * feature_ranges.values\n",
    "    \n",
    "    # Add noise only to synthetic samples\n",
    "    X_smote.loc[synthetic_indices] += noise\n",
    "    \n",
    "    return X_smote, y_smote"
   ],
   "id": "ce353bcffb6520b1",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-17T21:48:20.009918Z",
     "start_time": "2025-03-17T21:48:14.117361Z"
    }
   },
   "source": [
    "# Main function to run the augmentation experiment\n",
    "def run_augmentation_experiment(input_file):\n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    X = df.drop(columns=[\"performance_class\"])\n",
    "    y = df[\"performance_class\"]\n",
    "    \n",
    "    # ---- SPLIT DATASET ----\n",
    "    print(\"Splitting dataset into train/test sets...\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    \n",
    "    # Save split datasets for consistent splits\n",
    "    print(\"Saving train/test splits...\")\n",
    "    joblib.dump(X_train, \"../data/splits/X_train.pkl\")\n",
    "    joblib.dump(y_train, \"../data/splits/y_train.pkl\")\n",
    "    joblib.dump(X_test, \"../data/splits/X_test.pkl\")\n",
    "    joblib.dump(y_test, \"../data/splits/y_test.pkl\")\n",
    "    \n",
    "    print(\"Train/test split saved successfully!\")\n",
    "    \n",
    "    # Create a DataFrame from the training data for correlation analysis\n",
    "    df_train = pd.DataFrame(X_train, columns=X.columns)\n",
    "    df_train[\"performance_class\"] = y_train\n",
    "    \n",
    "    # Save original training data correlation matrix\n",
    "    original_corr = plot_correlation_heatmap(\n",
    "        X_train, \n",
    "        \"Original Training Data Correlation Matrix\", \n",
    "        \"../data/figures/original_train_correlation_heatmap.pdf\"\n",
    "    )\n",
    "    \n",
    "    # Define augmentation methods\n",
    "    augmentation_methods = {\n",
    "        'SMOTE_k5': SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42),\n",
    "        'BorderlineSMOTE': BorderlineSMOTE(sampling_strategy='auto', random_state=42),\n",
    "        'KMeansSMOTE': KMeansSMOTE(sampling_strategy='auto', cluster_balance_threshold=0.1, random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    augmented_dfs = {}\n",
    "    \n",
    "    # Run each augmentation method - ONLY ON TRAINING DATA\n",
    "    for method_name, augmenter in augmentation_methods.items():\n",
    "        print(f\"\\nRunning {method_name} on training data only...\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Initial augmentation to balance classes (using only training data)\n",
    "            X_train_resampled_1, y_train_resampled_1 = apply_smote_with_noise(\n",
    "                X_train, y_train, augmenter, noise_level=0.05, random_state=42\n",
    "            )\n",
    "            df_train_balanced = pd.DataFrame(X_train_resampled_1, columns=X_train.columns)\n",
    "            df_train_balanced[\"performance_class\"] = y_train_resampled_1\n",
    "                        \n",
    "            print(f\"Training dataset size after initial {method_name}:\", df_train_balanced.shape)\n",
    "            print(y_train_resampled_1.value_counts())\n",
    "                        \n",
    "            # Direct large-scale augmentation to reach target size\n",
    "            target_count_per_class = 1000\n",
    "            sampling_strategy_large = {}\n",
    "            for cls in y_train_resampled_1.unique():\n",
    "                sampling_strategy_large[cls] = target_count_per_class\n",
    "                        \n",
    "            augmenter_params = {}\n",
    "            if method_name == 'KMeansSMOTE':\n",
    "                augmenter_params['cluster_balance_threshold'] = 0.1\n",
    "                        \n",
    "            if method_name == 'SMOTE_k5':\n",
    "                augmenter_large = SMOTE(sampling_strategy=sampling_strategy_large, k_neighbors=5, random_state=44, **augmenter_params)\n",
    "            elif method_name == 'BorderlineSMOTE':\n",
    "                augmenter_large = BorderlineSMOTE(sampling_strategy=sampling_strategy_large, random_state=44, **augmenter_params)\n",
    "            elif method_name == 'KMeansSMOTE':\n",
    "                augmenter_large = KMeansSMOTE(sampling_strategy=sampling_strategy_large, random_state=44, **augmenter_params)\n",
    "                        \n",
    "            X_train_balanced = df_train_balanced.drop(columns=[\"performance_class\"])\n",
    "            y_train_balanced = df_train_balanced[\"performance_class\"]\n",
    "            X_train_resampled_large, y_train_resampled_large = apply_smote_with_noise(\n",
    "                X_train_balanced, y_train_balanced, augmenter_large, noise_level=0.05, random_state=44\n",
    "            )\n",
    "                        \n",
    "            df_train_large = pd.DataFrame(X_train_resampled_large, columns=X_train.columns)\n",
    "            df_train_large[\"performance_class\"] = y_train_resampled_large\n",
    "                        \n",
    "            print(f\"Large training dataset size after {method_name}:\", df_train_large.shape)\n",
    "            print(y_train_resampled_large.value_counts())\n",
    "            \n",
    "            # Save the augmented training data as pkl files\n",
    "            joblib.dump(X_train_resampled_large, f\"../data/splits/X_train_augmented_{method_name}.pkl\")\n",
    "            joblib.dump(y_train_resampled_large, f\"../data/splits/y_train_augmented_{method_name}.pkl\")\n",
    "            print(f\"Augmented training data saved as pkl files for {method_name}\")\n",
    "                        \n",
    "            # Save the augmented correlation matrix\n",
    "            augmented_corr = plot_correlation_heatmap(\n",
    "                df_train_large.drop(columns=[\"performance_class\"]),\n",
    "                f\"{method_name} Augmented Training Data Correlation Matrix\",\n",
    "                f\"../data/figures/{method_name}_train_correlation_heatmap.pdf\"\n",
    "            )\n",
    "                        \n",
    "            # Calculate correlation difference metrics\n",
    "            diff_results = calculate_correlation_difference(original_corr, augmented_corr)\n",
    "                        \n",
    "            # Save results\n",
    "            results[method_name] = diff_results\n",
    "            augmented_dfs[method_name] = df_train_large\n",
    "                        \n",
    "            # Plot the difference matrix\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            heatmap = sns.heatmap(\n",
    "                diff_results['diff_matrix'],\n",
    "                mask=np.triu(np.ones_like(diff_results['diff_matrix'], dtype=bool), k=1),\n",
    "                annot=True,\n",
    "                fmt=\"\",\n",
    "                annot_kws={\"size\": 8},\n",
    "                cmap=custom_cmap,\n",
    "                vmin=0,\n",
    "                vmax=0.5\n",
    "            )\n",
    "            # Apply custom formatter\n",
    "            for text in heatmap.texts:\n",
    "                text_value = float(text.get_text().replace('−', '-'))\n",
    "                text.set_text(custom_fmt(text_value))\n",
    "            plt.title(f'Correlation Difference: {method_name}\\nMean: {diff_results[\"mean_abs_diff\"]:.4f}, Max: {diff_results[\"max_abs_diff\"]:.4f}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"../data/figures/{method_name}_diff_heatmap.pdf\", dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {method_name}: {str(e)}\")\n",
    "    \n",
    "    # Save correlation difference metrics to CSV\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Method': list(results.keys()),\n",
    "        'Mean_Absolute_Difference': [results[method]['mean_abs_diff'] for method in results],\n",
    "        'Max_Absolute_Difference': [results[method]['max_abs_diff'] for method in results]\n",
    "    })\n",
    "    \n",
    "    metrics_df.to_csv(\"../data/correlation_difference_metrics.csv\", index=False)\n",
    "    print(\"\\nCorrelation difference metrics saved to 'correlation_difference_metrics.csv'\")\n",
    "    \n",
    "    # Find the best method\n",
    "    # Find the best method\n",
    "    if results:\n",
    "        best_method = min(results.items(), key=lambda x: x[1]['mean_abs_diff'])[0]\n",
    "        print(f\"\\nBest method based on correlation preservation: {best_method}\")\n",
    "        \n",
    "        # Plot feature distributions for the best method\n",
    "        if best_method in augmented_dfs:\n",
    "            # Compare original training data with augmented training data\n",
    "            df_train_for_plot = pd.DataFrame(X_train, columns=X_train.columns)\n",
    "            df_train_for_plot[\"performance_class\"] = y_train\n",
    "            \n",
    "            plot_feature_distributions(df_train_for_plot, augmented_dfs[best_method], best_method)\n",
    "            print(f\"Feature distribution plot saved for {best_method}\")\n",
    "            \n",
    "            # Extract X and y from the best augmented DataFrame\n",
    "            best_df = augmented_dfs[best_method]\n",
    "            X_train_augmented_best = best_df.drop(columns=[\"performance_class\"])\n",
    "            y_train_augmented_best = best_df[\"performance_class\"]\n",
    "            \n",
    "            # Save as pickle files\n",
    "            joblib.dump(X_train_augmented_best, f\"../data/splits/X_train_augmented_best.pkl\")\n",
    "            joblib.dump(y_train_augmented_best, f\"../data/splits/y_train_augmented_best.pkl\")\n",
    "            print(f\"Best augmented training data ({best_method}) saved as pickle files\")\n",
    "    \n",
    "    return results, augmented_dfs\n",
    "\n",
    "# Run the experiment\n",
    "if __name__ == \"__main__\":\n",
    "    results, augmented_dfs = run_augmentation_experiment(\"../data/features/filtered_labeled_feature_matrix.csv\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Splitting dataset into train/test sets...\n",
      "Saving train/test splits...\n",
      "Train/test split saved successfully!\n",
      "\n",
      "Running SMOTE_k5 on training data only...\n",
      "Training dataset size after initial SMOTE_k5: (482, 26)\n",
      "performance_class\n",
      "1    241\n",
      "0    241\n",
      "Name: count, dtype: int64\n",
      "Large training dataset size after SMOTE_k5: (2000, 26)\n",
      "performance_class\n",
      "1    1000\n",
      "0    1000\n",
      "Name: count, dtype: int64\n",
      "Augmented training data saved as pkl files for SMOTE_k5\n",
      "\n",
      "Running BorderlineSMOTE on training data only...\n",
      "Training dataset size after initial BorderlineSMOTE: (482, 26)\n",
      "performance_class\n",
      "1    241\n",
      "0    241\n",
      "Name: count, dtype: int64\n",
      "Large training dataset size after BorderlineSMOTE: (2000, 26)\n",
      "performance_class\n",
      "1    1000\n",
      "0    1000\n",
      "Name: count, dtype: int64\n",
      "Augmented training data saved as pkl files for BorderlineSMOTE\n",
      "\n",
      "Running KMeansSMOTE on training data only...\n",
      "Training dataset size after initial KMeansSMOTE: (486, 26)\n",
      "performance_class\n",
      "1    245\n",
      "0    241\n",
      "Name: count, dtype: int64\n",
      "Large training dataset size after KMeansSMOTE: (2008, 26)\n",
      "performance_class\n",
      "1    1004\n",
      "0    1004\n",
      "Name: count, dtype: int64\n",
      "Augmented training data saved as pkl files for KMeansSMOTE\n",
      "\n",
      "Correlation difference metrics saved to 'correlation_difference_metrics.csv'\n",
      "\n",
      "Best method based on correlation preservation: SMOTE_k5\n",
      "Feature distribution plot saved for SMOTE_k5\n",
      "Best augmented training data (SMOTE_k5) saved as pickle files\n"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
